---
layout: post
title: "Understanding Shuffle Partitions in Spark"
date: 2024-11-27
author: "Ameen Abdelmutalab"
tags: [Spark, Shuffle Partitions, Big Data, Performance Tuning]
image: "/assets/img/Databricks.png"
categories: Databricks
---

## Understanding Narrow and Wide Transformations

Before diving into shuffle partitions, it’s essential to understand the difference between **narrow** and **wide** transformations.

### Narrow Transformations
- Examples: `select`, `drop`, `filter`, etc.
- All the data needed for computation resides within a single partition.
- No shuffling of data is required, making these operations relatively inexpensive.

### Wide Transformations
- Examples: `groupBy`, `distinct`, `orderBy`, etc.
- Data needs to be aggregated or redistributed across partitions to compute global values.
- Requires a **shuffle**, which is a costly operation where data is moved between executors.

#### Why Are Wide Transformations Expensive?
Wide transformations force data to be reorganized and sent over the network, resulting in:
- Increased I/O and network overhead.
- Additional stages in the execution plan.
- A need to carefully tune configurations, such as the number of shuffle partitions.


## What Are Shuffle Partitions?

A **shuffle partition** is a logical division of data created during shuffle operations. Shuffle occurs when data needs to be redistributed across partitions, which is common in wide transformations like `groupBy` and `join`.

### Default Shuffle Partitions
By default, Spark sets the number of shuffle partitions to **200**, regardless of the size of the dataset. This default may not be optimal:
- **Small datasets**: 200 partitions can lead to excessive scheduling overhead.
- **Large datasets**: 200 partitions may result in large partition sizes, causing memory bottlenecks.
---

## Impact of Shuffle Partitions on Performance

### Why Tune Shuffle Partitions?
1. **Parallelism**: The number of shuffle partitions determines the degree of parallelism in the shuffle stage. More partitions generally mean better utilization of cluster resources.
2. **Resource Utilization**: Uneven or suboptimal partition sizes can lead to resource contention or idle executors.
3. **Task Efficiency**: Too few partitions result in overloaded tasks, while too many partitions lead to increased task scheduling overhead.

### Real-World Example
Let’s consider a `groupBy` operation to count the most common call types in a dataset:
```sql
SELECT call_type, COUNT(*) 
FROM fire_calls 
GROUP BY call_type;
```
This groupBy triggers a wide transformation, creating shuffle partitions:

- By default, 200 shuffle partitions are used.
- For small datasets, this results in tasks that are too small.
- For large datasets, this can lead to tasks that are too large and slow to process.

### How to Configure Shuffle Partitions
#### Setting Shuffle Partitions
There are two main ways to configure shuffle partitions:

1. At Cluster Creation:

- Set the default value in the Spark configuration for the cluster.
- All notebooks attached to the cluster will inherit this value.

```python
spark.conf.set("spark.sql.shuffle.partitions", 100)
```
2. Within a Notebook:

- Dynamically adjust shuffle partitions for specific workloads.
- This is ideal for interactive exploration and iterative tuning.

```python
spark.conf.set("spark.sql.shuffle.partitions", 50)
```

Example: Adjusting Shuffle Partitions

```python
# Default shuffle partitions (200)
spark.conf.set("spark.sql.shuffle.partitions", 200)

# Optimize shuffle partitions for small data
spark.conf.set("spark.sql.shuffle.partitions", 10)

# Optimize shuffle partitions for large data
spark.conf.set("spark.sql.shuffle.partitions", 500)
```

## Best Practices for Tuning Shuffle Partitions
1. Adjust Based on Data Size:

- Aim for partition sizes between 128 MB and 200 MB.
- Use fewer partitions for small datasets and more partitions for large datasets.

2. Balance with Cluster Resources:

- Match the number of shuffle partitions to the available cores in the cluster.
- A good starting point is 2x to 3x the total number of cores.

3. Experiment with Values:

- Test different values for spark.sql.shuffle.partitions.
- Monitor runtime and resource usage using the Spark UI.

4. Monitor Partition Sizes:

- Check partition sizes in the Spark UI to detect uneven distribution or skewed data.

## Leveraging Adaptive Query Execution (AQE)
Starting with Spark 3.0, Adaptive Query Execution (AQE) allows Spark to dynamically adjust shuffle partitions at runtime based on the size of intermediate data.

### How to Enable AQE

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

### Benefits of AQE

1. Dynamic Partitioning:

- Adjusts the number of shuffle partitions automatically.
- Reduces the need for manual tuning.

2. Handling Skewed Data:

- Splits skewed partitions into smaller chunks to balance the workload.

3. Improved Performance:

- Minimizes shuffle overhead and optimizes resource usage.

### Example Workflow with Shuffle Partitions

#### Without Tuning
```python
# Default configuration
spark.conf.set("spark.sql.shuffle.partitions", 200)

# Wide transformation (groupBy)
df.groupBy("call_type").count().show()
```
- Results in 200 shuffle partitions, which may not be optimal.

#### With Tuning

```python
# Adjust shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", 50)

# Rerun the same query
df.groupBy("call_type").count().show()
```

- Results in fewer shuffle partitions, reducing overhead and improving runtime.


### Common Pitfalls and Solutions

1. Underpartitioning:

- Causes memory bottlenecks and straggler tasks.
- Solution: Increase the number of shuffle partitions.

2. Overpartitioning:

- Leads to excessive scheduling overhead.
- Solution: Decrease the number of shuffle partitions.

3. Data Skew:

- Uneven partition sizes cause some tasks to take significantly longer.
- Solution: Use AQE or apply techniques like salting to redistribute data.

### Monitoring Shuffle Partitions Using Spark UI

1. Jobs Tab:

- View stages and tasks for each job.
- Check for uneven task durations or excessive shuffle reads/writes.

2. Stages Tab:

- Monitor the number of shuffle partitions and their sizes.
- Detect skewed data or underutilized partitions.

3. Storage Tab:

- Analyze cached data and partition sizes for cached datasets.








