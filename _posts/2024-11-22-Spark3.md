---
layout: post
title: "PySpark DataFrame Operations and Examples"
date: 2024-11-23
author: "Ameen Abdelmutalab"
tags: [data science, PySpark, Spark, Big Data]
image: "/assets/img/spark1.png"
categories: Spark
---

## Building on PySpark Basics: Advanced DataFrame Operations

In the previous blog, we introduced PySpark and its DataFrame basics, covering how to load data, inspect schemas, and display rows. This blog takes those concepts further by exploring advanced DataFrame operations, such as:
- Working with schema inference.
- Viewing and analyzing columns and data types.
- Performing complex column operations like renaming, adding, and dropping.
- Filtering, sorting, and grouping data.
- Generating descriptive statistics.

We'll use the following dataset for our examples. Here's a sample of the data:

| EmployeeID | Name     | Department | Salary  | YearsExperience |
|------------|----------|------------|---------|-----------------|
| 101        | Alice    | HR         | 50000   | 3               |
| 102        | Bob      | IT         | 80000   | 5               |
| 103        | Charlie  | Marketing  | 60000   | 4               |



## Reading Datasets with Schema Inference

To load a dataset into a DataFrame, PySpark provides powerful methods to automatically detect and assign column data types using `inferSchema`. This ensures your DataFrame is structured correctly for analysis.

```
df_pyspark = spark.read.option('header', 'true').csv('employees.csv', inferSchema=True)
```

Key Parameters:

- header='true': Specifies that the first row contains column headers.
- inferSchema=True: Automatically detects data types.
To verify the schema:

```
df_pyspark.printSchema()
```

Output:

```
root
 |-- EmployeeID: integer (nullable = true)
 |-- Name: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- Salary: integer (nullable = true)
 |-- YearsExperience: integer (nullable = true)
```
## Exploring Columns and Data Types
### Viewing Column Names

To view all column names, use the `.columns` attribute:

```
df_pyspark.columns
```
Output:

```
['EmployeeID', 'Name', 'Department', 'Salary', 'YearsExperience']
```

### Viewing Column Data Types

To explore specific columns, use the `select()` method:

```
df_pyspark.select('Name', 'Department').show()
```
Output:

```
+----------+------------+
|      Name|  Department|
+----------+------------+
|     Alice|          HR|
|       Bob|          IT|
|   Charlie|   Marketing|
+----------+------------+
```

### Filtering Rows
Use the `filter()` method to extract subsets of data:

```
# Filter employees with Salary greater than 60000
filtered_df = df_pyspark.filter(df_pyspark['Salary'] > 60000)
filtered_df.show()
```

Output:

```
+----------+----------+------------+-------+-----------------+
|EmployeeID|      Name|  Department| Salary| YearsExperience |
+----------+----------+------------+-------+-----------------+
|       102|       Bob|          IT|  80000|               5 |
+----------+----------+------------+-------+-----------------+
```

You can also apply multiple conditions:

```
# Filter employees in IT with more than 4 years of experience
filtered_df = df_pyspark.filter((df_pyspark['Department'] == 'IT') & (df_pyspark['YearsExperience'] > 4))
filtered_df.show()
```
Output:

```
+----------+----+------------+-------+-----------------+
|EmployeeID|Name|  Department| Salary| YearsExperience |
+----------+----+------------+-------+-----------------+
|       102| Bob|          IT|  80000|               5 |
+----------+----+------------+-------+-----------------+
```


## Sorting and Grouping Data
### Sorting Data
To sort rows, use the `orderBy()` method:

```
# Sort employees by Salary in descending order
df_pyspark.orderBy(df_pyspark['Salary'].desc()).show()
```
Output:

```
+----------+----------+------------+-------+-----------------+
|EmployeeID|      Name|  Department| Salary| YearsExperience |
+----------+----------+------------+-------+-----------------+
|       102|       Bob|          IT|  80000|               5 |
|       103|   Charlie|   Marketing|  60000|               4 |
|       101|     Alice|          HR|  50000|               3 |
+----------+----------+------------+-------+-----------------+
```
## Grouping and Aggregations
For summarizing data, use `groupBy()` with aggregation functions like `count()` or `avg()`:

```
# Group by Department and calculate average salary
df_pyspark.groupBy('Department').avg('Salary').show()
```

Output:

```
+------------+------------------+
|  Department|       avg(Salary)|
+------------+------------------+
|          HR|           50000.0|
|          IT|           80000.0|
|   Marketing|           60000.0|
+------------+------------------+
```

## Adding and Transforming Columns
### Adding a New Column
The `withColumn()` method allows you to add or modify columns. For example, adding a column for "Bonus":

```
df_pyspark = df_pyspark.withColumn('Bonus', df_pyspark['Salary'] * 0.1)
df_pyspark.show()
```
Output:

```
+----------+----------+------------+-------+-----------------+------+
|EmployeeID|      Name|  Department| Salary| YearsExperience |Bonus |
+----------+----------+------------+-------+-----------------+------+
|       101|     Alice|          HR|  50000|               3 | 5000 |
|       102|       Bob|          IT|  80000|               5 | 8000 |
|       103|   Charlie|   Marketing|  60000|               4 | 6000 |
+----------+----------+------------+-------+-----------------+------+
```
## Renaming Columns
You can rename a column using the `withColumnRenamed()` method:

```
df_pyspark = df_pyspark.withColumnRenamed('Name', 'Employee Name')
df_pyspark.show()
```
Output:

```
+----------+---------------+------------+-------+-----------------+
|EmployeeID|  Employee Name|  Department| Salary| YearsExperience |
+----------+---------------+------------+-------+-----------------+
|       101|          Alice|          HR|  50000|               3 |
|       102|            Bob|          IT|  80000|               5 |
|       103|        Charlie|   Marketing|  60000|               4 |
+----------+---------------+------------+-------+-----------------+
```
## Dropping Columns
To remove a column, use the `drop()` method:

```
df_pyspark = df_pyspark.drop('Bonus')
df_pyspark.show()
```
Output:

```
+----------+---------------+------------+-------+-----------------+
|EmployeeID|  Employee Name|  Department| Salary| YearsExperience |
+----------+---------------+------------+-------+-----------------+
|       101|          Alice|          HR|  50000|               3 |
|       102|            Bob|          IT|  80000|               5 |
|       103|        Charlie|   Marketing|  60000|               4 |
+----------+---------------+------------+-------+-----------------+
```
## Descriptive Statistics and Summary
Using `describe()`
To compute summary statistics for numerical columns:

```
df_pyspark.describe().show()
```
Output:

```
+-------+------------------+-------+-----------------+
|summary|       EmployeeID| Salary| YearsExperience |
+-------+------------------+-------+-----------------+
|  count|                 3|      3|                3|
|   mean|102.0             | 63333|             4.0 |
| stddev|  1.0             |15000.0|             1.0 |
|    min|101               | 50000|                3|
|    max|103               | 80000|                5|
+-------+------------------+-------+-----------------+
```
