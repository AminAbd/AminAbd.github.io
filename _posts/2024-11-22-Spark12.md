---
layout: post
title: "Optimizing Performance with Caching in Databricks"
date: 2024-11-27
author: "Ameen Abdelmutalab"
tags: [Databricks, Spark, Caching, Big Data, Performance Optimization]
image: "/assets/img/DatabricksCaching.png"
categories: Spark
---

## Introduction

Caching is one of the most powerful tools in Databricks for improving performance when working with large datasets. By temporarily storing intermediate data in memory, caching minimizes the need to repeatedly recompute or re-read data from external storage, significantly speeding up operations. In this blog, weâ€™ll explore caching in Databricks, how to implement it, and best practices to make the most of this feature.

---

## Why Caching Matters

Caching is especially useful in scenarios where datasets are reused across multiple operations. Some common use cases include:

- Iterative machine learning algorithms.
- Interactive queries on large datasets.
- Repeated aggregations or joins on the same dataset.

### Benefits of Caching
1. **Reduced Latency**: Speeds up operations by avoiding redundant computations.
2. **Efficient Resource Utilization**: Frees up cluster resources by minimizing I/O operations.
3. **Improved Workflow Productivity**: Allows users to quickly iterate on queries and transformations.

---

## Types of Caching in Databricks

### 1. **`cache()` Method**
Databricks allows you to cache DataFrames or tables in memory using the `cache()` method. This method stores data in memory across all nodes in the cluster.

### 2. **`persist()` Method**
The `persist()` method offers greater control over how data is cached. You can choose different storage levels, such as:

- **MEMORY_AND_DISK** (default): Stores data in memory; spills to disk if memory is insufficient.
- **DISK_ONLY**: Stores data on disk only.
- **MEMORY_AND_DISK_SER**: Stores serialized data in memory; spills to disk if needed.
- **OFF_HEAP**: Stores data in off-heap memory (requires configuration).

---

## How to Use Caching in Databricks

### Caching a DataFrame
You can cache a DataFrame to speed up subsequent operations.

```python
# Load a DataFrame
df = spark.read.csv("dbfs:/path/to/large-dataset.csv", header=True, inferSchema=True)

# Cache the DataFrame
df.cache()

# Perform operations
df.select("column1").show()
```

### Persisting a DataFrame
For more control over storage, use the `persist()` method:

```python
from pyspark import StorageLevel

# Persist the DataFrame with MEMORY_AND_DISK level
df.persist(StorageLevel.MEMORY_AND_DISK)

# Perform operations
df.groupBy("column1").count().show()
```

### Unpersisting Data
When cached data is no longer needed, unpersist it to free up memory and disk space.

```python
# Unpersist the DataFrame
df.unpersist()
```

---

## Caching SQL Tables in Databricks

Databricks also supports caching tables using SQL commands.

### SQL Example
```sql
CACHE TABLE my_cached_table
OPTIONS ('storageLevel' 'MEMORY_AND_DISK')
AS SELECT * FROM my_large_table;
```

#### CACHE TABLE my_cached_table
This part of the command creates a cached version of the query result and assigns it the name my_cached_table. The cached table is stored in memory (RAM) for faster access. If memory is insufficient, it will use disk storage as a fallback.

my_cached_table is a temporary cached representation of the query result.
The table is materialized the first time it is queried, meaning the query is executed, and the result is stored in the cache.
### Querying a Cached Table
```sql
SELECT * FROM my_cached_table LIMIT 10;
```
The query triggers caching, and the result is stored in memory/disk for future queries.

### Verifying Cached Data
You can verify if data is cached using SQL or the Spark UI.

```sql
SHOW CACHES;
```
This command lists all cached tables, their storage levels, and memory/disk usage.

### Using Spark UI
Open the Spark UI from the Databricks Cluster Details page.
Navigate to the Storage Tab to view cached datasets, their storage levels, and sizes.
### Clearing Cached Data
To free up memory or disk resources:

- Uncache a Specific Table:

```sql
UNCACHE TABLE my_cached_table;
```

- Clear All Cached Data:

```python
spark.catalog.clearCache()
```
```sql
CLEAR CACHE;
```
### Understanding Lazy Caching
Caching in Spark is lazy:

When you issue a CACHE TABLE or cache() command, Spark marks the dataset for caching but does not immediately load it into memory.
The data is cached only when the dataset or table is queried, such as through a SELECT or a transformation.
For example:

```sql
CACHE TABLE my_cached_table;
SELECT * FROM my_cached_table LIMIT 10;
```
The SELECT statement triggers the actual caching process.

### Monitoring Cached Data Using Spark UI
The Spark UI provides detailed insights into cached data:

- Storage Tab: View cached datasets, their sizes, and memory/disk usage.
- Task Execution Details: Monitor reduced I/O and shuffle operations for cached tasks.

Use the Spark UI to identify:

- Over-caching or underutilized cached datasets.
- Large datasets spilling to disk due to insufficient memory.

### Example Workflow with Caching
1. Load Data:

```python
df = spark.read.csv("dbfs:/path/to/large-dataset.csv", header=True, inferSchema=True)
```
2. Cache the Data:

```python
df.cache()
```
3. Perform Transformations:

```python
df.groupBy("column1").agg({"column2": "avg"}).show()
df.filter(df.column3 > 100).select("column4").show()
```
4. Verify the Cache:

```sql
SHOW CACHES;
```
5. Unpersist Cached Data:

```python
df.unpersist()
```
6. Clear All Caches:

```python
spark.catalog.clearCache()
```
## Best Practices for Caching in Databricks
1. Cache Only When Necessary: Avoid caching datasets that are not reused.
2. Use MEMORY_AND_DISK for Large Datasets: Ensures resilience if memory is insufficient.
3. Monitor Cache Usage: Regularly review memory/disk usage in Spark UI.
4. Unpersist Cached Data: Always unpersist data that is no longer needed.
5. Avoid Over-Caching: Over-caching can lead to memory contention and degrade performance.
## Common Pitfalls and Solutions
1. Caching Too Much Data:

- Problem: Excessive memory and disk usage.
- Solution: Cache only frequently reused datasets.

2. Forgetting to Unpersist:

- Problem: Resources remain occupied unnecessarily.
- Solution: Always call unpersist() after processing.

3. Small Datasets:

- Problem: Adds unnecessary overhead without performance gains.
- Solution: Use caching for large datasets or computationally expensive queries.
