---
layout: post
title: "Handling Missing Values in PySpark"
date: 2024-11-25
author: "Ameen Abdelmutalab"
tags: [data science, PySpark, Spark, Big Data]
image: "/assets/img/spark_dataframe_adv.png"
categories: Spark
---
## Introduction

In this post, we explore how to handle missing values in PySpark, an essential task for maintaining data quality in big data workflows. Building on the concepts from the previous blogs, we will demonstrate practical techniques for managing missing data, including:

- Dropping columns
- Dropping rows
- Exploring parameters for dropping functionalities
- Handling missing values using statistical measures (mean, median, and mode)

---

## PySpark DataFrame Example

For this blog, we’ll continue with the `employees.csv` dataset:

```
EmployeeID,Name,Department,Salary,YearsExperience
101,John,Engineering,70000,5
102,,Marketing,60000,
103,Alex,HR,,2
104,Emily,,80000,7
105,Michael,Engineering,90000,
```

Loading the Dataset

```
# Load the dataset
df_pyspark = spark.read.option('header', 'true').csv('employees.csv', inferSchema=True)

# Show the data
df_pyspark.show()
```

Output:

```
+----------+-------+------------+------+---------------+
|EmployeeID|   Name|  Department|Salary|YearsExperience|
+----------+-------+------------+------+---------------+
|       101|   John|Engineering | 70000|              5|
|       102|   null|   Marketing| 60000|           null|
|       103|   Alex|          HR|  null|              2|
|       104|  Emily|        null| 80000|              7|
|       105|Michael|Engineering | 90000|           null|
+----------+-------+------------+------+---------------+

```

# Dropping Columns with Missing Values

If a column contains too many missing values, it might make sense to drop it entirely. Use the `drop()` method for this:


```
# Drop the "Department" column
df_pyspark = df_pyspark.drop("Department")

# Show the updated DataFrame
df_pyspark.show()
```

Output:

```
+----------+-------+------+---------------+
|EmployeeID|   Name|Salary|YearsExperience|
+----------+-------+------+---------------+
|       101|   John| 70000|              5|
|       102|   null| 60000|           null|
|       103|   Alex|  null|              2|
|       104|  Emily| 80000|              7|
|       105|Michael| 90000|           null|
+----------+-------+------+---------------+
```

# Dropping Rows with Missing Values
To drop rows containing missing values, use the `dropna()` method. This method can be customized to drop rows with missing values based on specific conditions.

Example: Dropping Rows with Any Null Values

```
# Drop rows with any null values
df_pyspark_dropped = df_pyspark.dropna()

# Show the updated DataFrame
df_pyspark_dropped.show()
```

Output:

```
+----------+-------+------+---------------+
|EmployeeID|   Name|Salary|YearsExperience|
+----------+-------+------+---------------+
|       101|   John| 70000|              5|
|       104|  Emily| 80000|              7|
+----------+-------+------+---------------+
```

## Exploring Parameters in dropna()
The `dropna()` method allows more control with parameters such as how, thresh, and subset.

Drop Rows Only if All Columns Are Null
```
# Drop rows only if all columns are null
df_pyspark.dropna(how="all").show()
```

Drop Rows Based on a Threshold of Non-Null Values

```
# Drop rows with less than 2 non-null values
df_pyspark.dropna(thresh=2).show()
```

## Handling Missing Values with Statistical Measures

Instead of dropping data, we can impute missing values using statistical measures like `mean`, `median`, or `mode`. This ensures we don’t lose valuable information.

### Filling Missing Values with the Mean
To fill missing values in a numeric column with the mean:

```
from pyspark.sql.functions import mean
# Calculate the mean of the Salary column
mean_salary = df_pyspark.select(mean("Salary")).collect()[0][0]

# Fill missing values in the Salary column with the mean
df_pyspark = df_pyspark.fillna({"Salary": mean_salary})

# Show the updated DataFrame
df_pyspark.show()
```
Output:

```
+----------+-------+---------+---------------+
|EmployeeID|   Name|   Salary|YearsExperience|
+----------+-------+---------+---------------+
|       101|   John|  70000.0|              5|
|       102|   null|  60000.0|           null|
|       103|   Alex|  75000.0|              2|
|       104|  Emily|  80000.0|              7|
|       105|Michael|  90000.0|           null|
+----------+-------+---------+---------------+
```
### Filling Missing Values with the Median or Mode
PySpark does not have built-in methods for calculating the median or mode, but you can use custom approaches:

```
# Calculate median using approximate quantiles
median_salary = df_pyspark.approxQuantile("Salary", [0.5], 0.25)[0]

# Fill missing values in Salary with the median
df_pyspark = df_pyspark.fillna({"Salary": median_salary})
```
### Mode

For categorical data like the "Name" column, you can compute the mode manually and fill missing values.

## Imputing Missing Values with PySpark’s Imputer
PySpark provides the Imputer class as part of its MLlib feature engineering tools. This is a more efficient and scalable way to handle missing values in numeric columns.

Example: Using the Imputer for Multiple Columns

```
from pyspark.ml.feature import Imputer

# Define the Imputer
imputer = Imputer(
    inputCols=['YearsExperience', 'Salary'],
    outputCols=["{}_imputed".format(c) for c in ['YearsExperience', 'Salary']]
).setStrategy("mean")

# Apply the Imputer to the DataFrame
df_pyspark_imputed = imputer.fit(df_pyspark).transform(df_pyspark)

# Show the updated DataFrame with imputed values
df_pyspark_imputed.show()
```
Output:

```
+----------+-------+------+---------------+--------------------+-----------------+
|EmployeeID|   Name|Salary|YearsExperience|YearsExperience_imputed|   Salary_imputed|
+----------+-------+------+---------------+--------------------+-----------------+
|       101|   John| 70000|              5|                   5.0|         70000.0|
|       102|   null| 60000|           null|                   4.7|         60000.0|
|       103|   Alex|  null|              2|                   2.0|         75000.0|
|       104|  Emily| 80000|              7|                   7.0|         80000.0|
|       105|Michael| 90000|           null|                   4.7|         90000.0|
+----------+-------+------+---------------+--------------------+-----------------+
```
Here:

- inputCols specifies the columns to impute.
- outputCols defines the new column names with imputed values.
- setStrategy("mean") specifies the imputation strategy (e.g., mean, median).


