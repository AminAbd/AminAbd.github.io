---
layout: post
title: "Understanding File Formats and Compression in Distributed Systems"
date: 2024-12-02
author: "Ameen Abdelmutalab"
tags: [File Formats, Parquet, Delta, Compression, Spark, Big Data]
image: "/assets/img/Databricks.png"
categories: Databricks
---

## Introduction

In the world of distributed systems and big data, selecting the right file format and compression strategy can dramatically impact performance and efficiency. While CSV files are commonly encountered, they’re often not the optimal choice for big data processing. Advanced formats like **Parquet** and **Delta** offer better performance by leveraging columnar storage and compression. In this blog, we’ll dive into various file formats, their compression techniques, and how to choose the best option based on your use case.

---

## Common File Formats in Big Data

### 1. **CSV (Comma-Separated Values)**
CSV files store data in a simple row-based format, where each row is a record, and columns are separated by delimiters like commas or colons. While easy to read and write, CSV files come with limitations:
- Inefficient storage and memory usage.
- Lack of built-in metadata (e.g., data types).
- Cannot handle advanced features like parallel processing efficiently.

#### Example:
```text
Name, Age, Location
Alice, 30, New York
Bob, 25, San Francisco
```

### 2. **Parquet**
**Parquet** is a columnar storage format designed for distributed systems like Spark. Instead of storing data row by row, Parquet stores data column by column. This enables better compression and faster queries, especially when working with subsets of columns.

#### Benefits of Parquet:
- **Efficient Compression**: Repeated values in columns (e.g., IDs) are stored once with pointers, reducing storage size.
- **Splittable Files**: Parquet files can be read in parallel, improving performance in distributed environments.
- **Metadata Storage**: Parquet stores schema information, so Spark can infer data types automatically.

### 3. **Delta**
**Delta** is an extension of Parquet that adds ACID transaction support, enabling database-like operations such as updates and deletes. It is built on top of Parquet and is ideal for building lakehouses.

#### Key Features of Delta:
- **ACID Transactions**: Ensures consistency in data modifications.
- **Schema Evolution**: Allows schema updates without breaking existing data.
- **Delta Logs**: Maintains metadata for data integrity.

---

## File Compression Techniques

Compression reduces file sizes and saves storage space, but it can also affect performance. Let’s compare two popular compression formats: **Gzip** and **Bzip**.

### 1. **Gzip**
- **Advantages**: Smaller file sizes.
- **Disadvantages**: Files are not splittable, so they cannot be processed in parallel.

#### Example:
- Original File: 1.8 GB
- Gzipped File: 260 MB
- Processing Time: 36.16 seconds (single partition).

### 2. **Bzip**
- **Advantages**: Files are splittable, enabling parallel processing.
- **Disadvantages**: Slower compression and decompression.

#### Example:
- Bzipped File: 193 MB
- Processing Time: ~2 minutes (8 partitions).

### 3. **Snappy**
Snappy is a popular compression format used in Parquet and Delta files. It offers a balance between compression and decompression speed, making it suitable for distributed ecosystems.

---

## Comparing File Formats and Performance

### Example: Fire Calls Dataset
We experimented with the **Fire Calls dataset**, stored in multiple file formats and compression techniques. Below are the results:

| **File Format**  | **Compression** | **Size**  | **Read Time** | **Partitions** |
|-------------------|-----------------|-----------|---------------|----------------|
| CSV              | None            | 1.8 GB    | 36.67 sec     | 1              |
| Gzip             | Gzipped         | 260 MB    | 36.16 sec     | 1              |
| Bzip             | Bzipped         | 193 MB    | ~2 min        | 8              |
| Parquet          | Snappy          | ~1 MB     | 1.22 sec      | 8              |
| Delta            | Snappy          | ~1 MB     | ~1 sec        | 8              |

### Observations:
- **CSV**: Easy to work with but inefficient for storage and processing.
- **Parquet**: Significantly faster reads due to columnar storage and metadata.
- **Delta**: Adds transactional support to Parquet without sacrificing speed.

---

## Schema Inference and Data Types

When reading files, Spark can infer schemas to determine data types. However, this process can be time-consuming, especially for large datasets.

- **Without Schema Inference**: Faster but treats all data as strings.
- **With Schema Inference**: Slower (e.g., ~51 seconds for a 1.8 GB file) but accurately interprets data types.

### Example:
```python
# Reading without schema inference
df = spark.read.csv(path, header=True, sep=":")
# Reading with schema inference
df = spark.read.csv(path, header=True, sep=":", inferSchema=True)
```

### Handling Timestamps
Parsing timestamps can be challenging due to inconsistent formats. Custom parsers may be required to handle non-standard timestamp formats.

---

## Parquet and Delta in Distributed Systems

### Columnar Storage Benefits:
1. **Efficient Queries**:
   - Read only relevant columns for your analysis.
2. **Compression**:
   - Reduce storage size with techniques like Snappy.
3. **Parallelism**:
   - Splittable files enable faster processing across multiple nodes.

### Delta Enhancements:
- **ACID Transactions**: Safely modify data without affecting other users.
- **Repartitioning**: Optimize parallelism by dividing data into smaller subsets.

#### Example: Writing to Delta
```python
df.write.format("delta").partitionBy("id").save("/mnt/delta-table")
```

---

## Choosing the Right Format

| **Scenario**                        | **Recommended Format** |
|-------------------------------------|-------------------------|
| Large-scale analytics               | Parquet or Delta        |
| Simple storage and sharing          | CSV                    |
| Frequent updates or transactions    | Delta                  |
| Space-saving requirements           | Parquet with Snappy     |
| Splittable file processing          | Parquet or Bzip         |

---

## Conclusion

Choosing the right file format and compression technique is critical for optimizing performance in distributed systems. While **CSV** is common and easy to use, **Parquet** and **Delta** offer significant advantages in scalability, compression, and parallel processing. Understanding the trade-offs between storage size and processing time helps you make informed decisions for your big data workflows. Explore these formats and compression techniques to elevate your data engineering practices!
``` 

This blog provides a comprehensive explanation of file formats, compression options, and their implications in distributed systems. It’s structured for clarity, with practical examples and recommendations for real-world use cases. Let me know if you’d like further adjustments!