---
layout: post
title: "Building an AI Web Scraper with Streamlit, Selenium, and Langchain"
date: 2024-11-12
author: "Ameen Abdelmutalab"
tags: [AI, web scraping, streamlit, selenium, langchain, python, tutorial]
image: "/assets/img/ai_scraper.png"
permalink: /projects/project-2/2024-11-12-blog-post-2/
---

In this tutorial, we’ll walk through creating an AI-powered web scraper using Python, combining tools like **Streamlit** for the interface, **Selenium** for web scraping, **BeautifulSoup** for cleaning up HTML, and **Langchain** with the **Ollama** model for parsing. This step-by-step guide will help you understand the workflow and core elements involved in setting up this AI-driven web scraper.

## Step 1: Install Required Packages

This project requires several libraries to handle tasks such as scraping, processing, and parsing content. Essential packages include Streamlit for the front-end interface, Selenium for scraping website content, BeautifulSoup for HTML cleaning, and Langchain with Ollama for natural language parsing. These packages are specified in a `requirements.txt` file, which can be installed to set up the environment quickly.

## Step 2: Creating the Streamlit User Interface

We begin by designing a simple interface using **Streamlit**. The interface:
1. Takes a URL input from the user.
2. Provides a button to trigger the scraping process.
3. Displays the cleaned content from the webpage in an expandable text area.
4. Allows users to input specific parsing instructions to retrieve targeted information from the scraped content.

Streamlit serves as an effective framework for quickly setting up web apps and provides an intuitive way for users to interact with the scraper.

## Step 3: Scraping the Webpage with Selenium and BeautifulSoup

Next, we use **Selenium** to load the webpage specified by the user and capture the full HTML source. Selenium launches a Chrome browser instance, navigates to the page, and retrieves the HTML content. After capturing the HTML, **BeautifulSoup** processes the HTML to extract the main content within the body tag, removing scripts, styles, and other unnecessary elements. The result is a cleaner, more readable version of the page’s content.

To handle larger web pages, the content is split into manageable chunks. This helps ensure smoother processing when we later pass the data to our parsing model.

## Step 4: Parsing Content with Langchain and Ollama

With the cleaned HTML content available, we use **Langchain**’s Ollama model to parse and extract specific data based on user instructions. This step relies on the following:
1. A structured prompt template that guides the model to focus only on the requested information, avoiding irrelevant details.
2. A chunked processing approach, where each chunk of content is processed individually to prevent memory overload and maximize the accuracy of parsed information.

The model is set up to follow instructions closely, ensuring it returns only the requested data without extra explanations. This allows for efficient and targeted extraction of information based on user needs.

## Running the Application

To run the project, ensure all required components, like **Chromedriver** for Selenium, are correctly installed. Once the environment is ready, launch the Streamlit app, enter a URL, describe the desired information, and trigger the scraping and parsing processes. The results are displayed in real-time, making it easy to retrieve and analyze specific content from the web.

## Conclusion

This AI web scraper offers a powerful, user-friendly tool for extracting and analyzing web content. By combining Streamlit, Selenium, BeautifulSoup, and Langchain, it demonstrates how modern NLP models can be integrated into a scraping pipeline, making targeted information extraction accessible for various applications.

## Acknowledgments

This project is based on concepts presented in a YouTube tutorial, which served as a foundation and inspiration for building this AI-powered web scraper. You can watch the original video [here]([https://www.youtube.com/watch?v=Oo8-nEuDBkk]).


## GitHub Repository

You can find the complete code for this project on [GitHub](https://github.com/yourusername/your-repo-name).

